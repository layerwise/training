{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Vorhersage von Immobilienpreisen\n",
    "\n",
    "## 1. Problemstellung\n",
    "\n",
    "Eine Investmentgesellschaft will seine internen Review- und Investment-Prozesse besser automatisieren.\n",
    "\n",
    "Teil des Portfolios der Investementgesellschaft sind Immobilienbestände im Gebiet um Ames, Iowa, USA. Über den Zustand und die Austattung dieser Immobilien wird selbstverständlich Buch geführt. Neben Wohnfläche, Baujahr, Zustand und Anzahl der Zimmer sind diverse andere Informationen vorhanden, so zum Beispiel die Form des Grundstücks, der Belag der Einfahrt, das Material der Außenwände und so weiter. Insgesamt sind für jede Immobilie in etwa ~80 Messgrößen und Eckdaten bekannt.\n",
    "\n",
    "Die Investmentgesellschaft hat ein Interesse daran, den Wert dieser Immobilien möglichst genau zu schätzen. Üblicherweise würde der Wert jeder Immobilie von Experten geschätzt. In einzelnen Fällen wäre dafür sogar eine Begutachtung des Objeckt nötig. Der Prozess, den Wert von fast 3000 Immobilien im Portfolio der Investmentgesellschaft zu schätzen ist langwierig, fehleranfällig und teuer.\n",
    "\n",
    "Deshalb ist die Investmentgesellschaft auf Sie zugekommen, um feststellen, ob es möglich ist, die Prozesse zu automatisieren, möglicherweise sogar durch *Machine Learning*.\n",
    "\n",
    "Der Kunde hat deshalb eine Beispielaufgabe für Sie vorbereitet, um das Potential von Methoden des *Machine Learning* für die Problemstellung einzuschätzen.\n",
    "\n",
    "Ihnen wir zunächst ein folgender Datensatz zur Verfügung gestellt:\n",
    "\n",
    "![Test Data](https://raw.githubusercontent.com/layerwise/training/main/assets/house_prices_test_example_image.png)\n",
    "\n",
    "Dabei handelt es sich um eine Liste von Immobilien im Bestand des Kunden, jede mit einer eindeutigen Identifikationsnummer, für die ein Verkaufspreis vorhergesagt werden soll. Für jede Immobilie sind diverse Messdaten und Informationen gegeben - insgesamt 80 solche Größen.\n",
    "\n",
    "Der Kunde hat per Expertenmeinung bereits eine Schätzung für den Verkaufspreis jeder dieser Immobilien angestellt - doch diese wird Ihnen nicht mitgeteilt. Ihre Aufgabe ist es, für jede der Immobilien einen Verkaufspreis vorherzusagen und dabei möglichst genau die Einschätzung des Kunden zu treffen.\n",
    "\n",
    "Das einzige, was Ihnen dafür zur Verfügung steht, ist ein weiterer Datensatz:\n",
    "\n",
    "![Train Data](https://raw.githubusercontent.com/layerwise/training/main/assets/house_prices_test_example_image.png)\n",
    "\n",
    "\n",
    "Dieser Datensatz ist sehr ähnlich dem ersten Datensatz. Er beschreibt eine andere Menge von Immobilien, die sich zuvor im Bestand des Kunden befunden haben und inzwischen verkauft wurden, für die die gleichen Messgrößen und Informationen vorliegen. Es gibt keine Überschneidung zwischen den zwei Datensätzen, d.h. jede Idenfikationsnummer in diesem zweiten Datensatz kommt nicht im ersten Datensatz vor und umgekehrt.\n",
    "\n",
    "Für diesen zweiten Datensatz gibt es aber eine zusätzliche Information: hier wurde bereits der tatsächliche Verkaufspreis (*SalePrice*) in US-Dollar angegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wie lassen sich die Informationen aus dem zweiten Datensatz nutzen, um für die Immobilien des Kunden den Verkaufspreis vorherzusagen?**\n",
    "\n",
    "**Schreiben Sie ein Programm, das für jede Immobilie des Kunden ein Zahl ausgibt - Ihre Schätzung für den Verkaufspreis in US-Dollar.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daten\n",
    "\n",
    "### 2.1. Daten laden\n",
    "\n",
    "Zuallerst müssen Sie die Daten für die folgende Aufgabe finden und herunterladen. Es handelt sich dabei um den Datensatz *Ames House Prices* der in der Lektion als zip-Archiv heruntergeladen werden kann. Nach dem Entpacken befinden sich im Ordner folgende Dateien:\n",
    "\n",
    "- `AmesIowaHousingData.csv`\n",
    "- `AmesIowaHousingData_new.csv`\n",
    "- `AmesIowaHousingDataDocumentation.txt`\n",
    "\n",
    "Beschäftigen Sie sich mit den Dateien. Zur Erklärung: die Datei `AmesIowaHousingData.csv` ist diejenige, die die Investmentgesellschaft Ihnen zum Trainieren Ihres Modells zur Verfügung gestellt hat. Die Datei `AmesIowaHousingData_new.csv` ist die Datei, auf der Sie eine Vorhersage anstellen sollen (die Investmentgesellschaft hat hier die tatsächlichen Preise der Immobilien gelöscht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Daten laden\n",
    "# TODO: Trainings- und Testdaten - im Idealfall sollten die Testdaten bis zum fertigen Modell\n",
    "# nicht einmal angeschaut werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Daten sichten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Zielvariable\n",
    "\n",
    "Zunächst die Zielvariable - `SalePrice`. Hiermit kann die Verteilung der Zielvariablen eingeschätzt werden. Oft sind Transformationen der Zielvariablen sinnvoll, z.B. logarithmische Transformationen oder Transformationen hin zu einer stärker normalverteilten Zielvariablen. Dies ist auch hier eine Möglichkeit, aber wir fassen dieses Thema dennoch als weiterführend auf und überspringen es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Korrelationen der Features mit der Zielvariablen\n",
    "\n",
    "**Numerische Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kategorische Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Korrelationsmatrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduzierte Korrelationsmatrix**\n",
    "\n",
    "Eine Korrelationsmatrix *kann* zur Selektion von Features verwendet werden. Dies ist nur eine von vielen Möglichkeiten und hat viele Limitationen. So werden durch eine Korrelationsmatrix nur lineare Korrelationen abgebildet. Nicht-lineare Modelle (z.B. RandomForests) können eventuell auch nicht-lineare Zusammenhänge entdecken und werden durch eine Selektion der Features eher eingeschränkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatterplot Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Missing Values\n",
    "\n",
    "Anhand der Trainingsdaten sollte sich ein Überblick über die fehlenden Werte verschafft werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Kategorische Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5. Numerische Features\n",
    "\n",
    "Um einzuschätzen, ob und welche Skalierung der numerischen Features notwendig ist, kann der folgende Plot helfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Das Machine Learning Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Vorbereitung, Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Fehlende Werte ersetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scikit-Learn Imputation\n",
    "# TODO: Imputations-Transformationen instanziieren und fitten\n",
    "# TODO: Trainingsdaten transformieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encoding: Ordinale und Nominale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scikit-Learn Feature Encoding\n",
    "# TODO: Encoding-Transformationen instanziieren und fitten\n",
    "# TODO: Trainingsdaten transformieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Numerische Features skalieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scikit-Learn Feature Scaling\n",
    "# TODO: Feature Scaling instanziieren und fitten\n",
    "# TODO: Trainingsdaten transformieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Skalierung überprüfen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modell und Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Scikit-Learn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vorhersage auf neuen Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Vorhersage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
