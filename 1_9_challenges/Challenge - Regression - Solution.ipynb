{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Vorhersage von Immobilienpreisen\n",
    "\n",
    "## Beschreibung\n",
    "\n",
    "Eine Investmentgesellschaft will seine internen Review- und Investment-Prozesse besser automatisieren.\n",
    "\n",
    "Teil des Portfolios der Investementgesellschaft sind Immobilienbestände im Gebiet um Ames, Iowa, USA. Über den Zustand und die Austattung dieser Immobilien wird selbstverständlich Buch geführt. Neben Wohnfläche, Baujahr, Zustand und Anzahl der Zimmer sind diverse andere Informationen vorhanden, so zum Beispiel die Form des Grundstücks, der Belag der Einfahrt, das Material der Außenwände und so weiter. Insgesamt sind für jede Immobilie in etwa ~80 Messgrößen und Eckdaten bekannt.\n",
    "\n",
    "Die Investmentgesellschaft hat ein Interesse daran, den Wert dieser Immobilien möglichst genau zu schätzen. Üblicherweise würde der Wert jeder Immobilie von Experten geschätzt. In einzelnen Fällen wäre dafür sogar eine Begutachtung des Objeckt nötig. Der Prozess, den Wert von fast 3000 Immobilien im Portfolio der Investmentgesellschaft zu schätzen ist langwierig, fehleranfällig und teuer.\n",
    "\n",
    "Deshalb ist die Investmentgesellschaft auf Sie zugekommen, um feststellen, ob es möglich ist, die Prozesse zu automatisieren, möglicherweise sogar durch *Machine Learning*.\n",
    "\n",
    "Der Kunde hat deshalb eine Beispielaufgabe für Sie vorbereitet, um das Potential von Methoden des *Machine Learning* für die Problemstellung einzuschätzen.\n",
    "\n",
    "Ihnen wir zunächst ein folgender Datensatz zur Verfügung gestellt:\n",
    "\n",
    "<img src=\"../assets/house_prices_test_example_image.png\" width=\"1000\" >\n",
    "\n",
    "Dabei handelt es sich um eine Liste von Immobilien im Bestand des Kunden, jede mit einer eindeutigen Identifikationsnummer, für die ein Verkaufspreis vorhergesagt werden soll. Für jede Immobilie sind diverse Messdaten und Informationen gegeben - insgesamt 80 solche Größen.\n",
    "\n",
    "Der Kunde hat per Expertenmeinung bereits eine Schätzung für den Verkaufspreis jeder dieser Immobilien angestellt - doch diese wird Ihnen nicht mitgeteilt. Ihre Aufgabe ist es, für jede der Immobilien einen Verkaufspreis vorherzusagen und dabei möglichst genau die Einschätzung des Kunden zu treffen.\n",
    "\n",
    "Das einzige, was Ihnen dafür zur Verfügung steht, ist ein weiterer Datensatz:\n",
    "\n",
    "<img src=\"../assets/house_prices_train_example_image.png\" width=\"1000\" >\n",
    "\n",
    "Dieser Datensatz ist sehr ähnlich dem ersten Datensatz. Er beschreibt eine andere Menge von Immobilien, die sich zuvor im Bestand des Kunden befunden haben und inzwischen verkauft wurden, für die die gleichen Messgrößen und Informationen vorliegen. Es gibt keine Überschneidung zwischen den zwei Datensätzen, d.h. jede Idenfikationsnummer in diesem zweiten Datensatz kommt nicht im ersten Datensatz vor und umgekehrt.\n",
    "\n",
    "Für diesen zweiten Datensatz gibt es aber eine zusätzliche Information: hier wurde bereits der tatsächliche Verkaufspreis (*SalePrice*) in US-Dollar angegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragestellung\n",
    "\n",
    "## Wie lassen sich die Informationen aus dem zweiten Datensatz nutzen, um für die Immobilien des Kunden den Verkaufspreis vorherzusagen ?\n",
    "\n",
    "## Schreiben Sie ein Programm, das für jede Immobilie des Kunden ein Zahl ausgibt - Ihre Schätzung für den Verkaufspreis in US-Dollar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Vorbereitung\n",
    "\n",
    "Laden Sie optional für zusätzliche Tipps und Anweisungen das Modul `learntools` aus dem Datenaustausch bzw. unter folgendem Google Drive-Link herunter ().\n",
    "\n",
    "Führen Sie dann den unten stehenden Code aus. Ändern Sie dazu die Variable `learntools_path` auf entsprechende Weise, sodass diese den Pfad speichert, an dem sich der heruntergeladene Ordner befindet.\n",
    "\n",
    "Mit dem Befehl\n",
    "\n",
    "```python\n",
    "aufgabe.check()\n",
    "```\n",
    "\n",
    "lassen sich nun für eine (hypothetische) Aufgabe `aufgabe` Lösungen abrufen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "learntools_path = f\"{os.getenv('HOME')}/alfatraining/learntools\"\n",
    "# learntools_path = \"path/to/learntools/folder\"\n",
    "sys.path.append(learntools_path)\n",
    "\n",
    "from learntools.core import binder; binder.bind(globals())\n",
    "from learntools.challenges.challenge2 import *\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten herunterladen und bereitstellen\n",
    "\n",
    "Zuallerst müssen Sie die Daten für die folgende Aufgabe finden und herunterladen. Diese befinden sich entweder im Datenaustausch oder sind unter folgendem Google Drive Link zu finden:\n",
    "https://drive.google.com/drive/folders/1v2A2afB0X6brczH54COvptKHzQ2O5lEg?usp=sharing\n",
    "\n",
    "Wie Sie die Daten in diesem Jupyter-Notebook verfügbar machen, hängt davon ab, ob Sie das Notebook lokal oder in Google Colab ausführen.\n",
    "\n",
    "Falls Sie das Notebook lokal ausführen, haben Sie zunächst nichts weiter zu tun. Platzieren Sie die heruntergeladene Datei in einem Ordner Ihrer Wahl und entpacken Sie sie, falls es sich um eine ZIP-Datei handelt.\n",
    "\n",
    "Falls Sie das Notebook in Google Colab ausführen, müssen Sie die Daten für das Notebook in der Cloud zugänglich machen - das Notebook hat keinen Zugriff auf Ihre lokalen Dateien. Folgen Sie diesem Link für zusätzliche Tipps zum Hochladen von Daten in Google Colab: https://towardsdatascience.com/importing-data-to-google-colab-the-clean-way-5ceef9e9e3c8. Falls Sie optional das Modul `learntools` von weiter oben verwenden wollen, müssen Sie dieses zusätzlich hochladen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst sollten Sie die beiden Datensätze kennenlernen und verstehen.\n",
    "Die gesamten Daten sind in zwei Dateien mit der Endung `.csv` abgespeichert.\n",
    "\n",
    "Die erste Liste ist\n",
    "- `AmesIowaHousingData_test.csv`\n",
    "\n",
    "Und die zweite Liste ist\n",
    "- `AmesIowaHousingData_train.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Daten laden\n",
    "\n",
    "Wie können Dateien mit der Endung .csv in Python geladen werden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wie können Dateien mit der Endung .csv in Python geladen werden?\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"../../data/challenge2\"\n",
    "\n",
    "train_data = pd.read_csv(f\"{data_path}/AmesIowaHousingData_train.csv\", index_col=0)\n",
    "test_data = pd.read_csv(f\"{data_path}/AmesIowaHousingData_test_with_labels.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Daten sichten\n",
    "\n",
    "Welche Analysen könnte man auf den Daten ausführen?\n",
    "\n",
    "Wie könnte man die Daten visualieren?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Daten vorverarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2344 entries, 39014 to 94319\n",
      "Data columns (total 81 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   PID              2344 non-null   int64  \n",
      " 1   MS SubClass      2344 non-null   int64  \n",
      " 2   MS Zoning        2344 non-null   object \n",
      " 3   Lot Frontage     1951 non-null   float64\n",
      " 4   Lot Area         2344 non-null   int64  \n",
      " 5   Street           2344 non-null   object \n",
      " 6   Alley            151 non-null    object \n",
      " 7   Lot Shape        2344 non-null   object \n",
      " 8   Land Contour     2344 non-null   object \n",
      " 9   Utilities        2344 non-null   object \n",
      " 10  Lot Config       2344 non-null   object \n",
      " 11  Land Slope       2344 non-null   object \n",
      " 12  Neighborhood     2344 non-null   object \n",
      " 13  Condition 1      2344 non-null   object \n",
      " 14  Condition 2      2344 non-null   object \n",
      " 15  Bldg Type        2344 non-null   object \n",
      " 16  House Style      2344 non-null   object \n",
      " 17  Overall Qual     2344 non-null   int64  \n",
      " 18  Overall Cond     2344 non-null   int64  \n",
      " 19  Year Built       2344 non-null   int64  \n",
      " 20  Year Remod/Add   2344 non-null   int64  \n",
      " 21  Roof Style       2344 non-null   object \n",
      " 22  Roof Matl        2344 non-null   object \n",
      " 23  Exterior 1st     2344 non-null   object \n",
      " 24  Exterior 2nd     2344 non-null   object \n",
      " 25  Mas Vnr Type     2323 non-null   object \n",
      " 26  Mas Vnr Area     2323 non-null   float64\n",
      " 27  Exter Qual       2344 non-null   object \n",
      " 28  Exter Cond       2344 non-null   object \n",
      " 29  Foundation       2344 non-null   object \n",
      " 30  Bsmt Qual        2284 non-null   object \n",
      " 31  Bsmt Cond        2284 non-null   object \n",
      " 32  Bsmt Exposure    2281 non-null   object \n",
      " 33  BsmtFin Type 1   2284 non-null   object \n",
      " 34  BsmtFin SF 1     2343 non-null   float64\n",
      " 35  BsmtFin Type 2   2283 non-null   object \n",
      " 36  BsmtFin SF 2     2343 non-null   float64\n",
      " 37  Bsmt Unf SF      2343 non-null   float64\n",
      " 38  Total Bsmt SF    2343 non-null   float64\n",
      " 39  Heating          2344 non-null   object \n",
      " 40  Heating QC       2344 non-null   object \n",
      " 41  Central Air      2344 non-null   object \n",
      " 42  Electrical       2343 non-null   object \n",
      " 43  1st Flr SF       2344 non-null   int64  \n",
      " 44  2nd Flr SF       2344 non-null   int64  \n",
      " 45  Low Qual Fin SF  2344 non-null   int64  \n",
      " 46  Gr Liv Area      2344 non-null   int64  \n",
      " 47  Bsmt Full Bath   2343 non-null   float64\n",
      " 48  Bsmt Half Bath   2343 non-null   float64\n",
      " 49  Full Bath        2344 non-null   int64  \n",
      " 50  Half Bath        2344 non-null   int64  \n",
      " 51  Bedroom AbvGr    2344 non-null   int64  \n",
      " 52  Kitchen AbvGr    2344 non-null   int64  \n",
      " 53  Kitchen Qual     2344 non-null   object \n",
      " 54  TotRms AbvGrd    2344 non-null   int64  \n",
      " 55  Functional       2344 non-null   object \n",
      " 56  Fireplaces       2344 non-null   int64  \n",
      " 57  Fireplace Qu     1191 non-null   object \n",
      " 58  Garage Type      2215 non-null   object \n",
      " 59  Garage Yr Blt    2213 non-null   float64\n",
      " 60  Garage Finish    2213 non-null   object \n",
      " 61  Garage Cars      2343 non-null   float64\n",
      " 62  Garage Area      2343 non-null   float64\n",
      " 63  Garage Qual      2213 non-null   object \n",
      " 64  Garage Cond      2213 non-null   object \n",
      " 65  Paved Drive      2344 non-null   object \n",
      " 66  Wood Deck SF     2344 non-null   int64  \n",
      " 67  Open Porch SF    2344 non-null   int64  \n",
      " 68  Enclosed Porch   2344 non-null   int64  \n",
      " 69  3Ssn Porch       2344 non-null   int64  \n",
      " 70  Screen Porch     2344 non-null   int64  \n",
      " 71  Pool Area        2344 non-null   int64  \n",
      " 72  Pool QC          9 non-null      object \n",
      " 73  Fence            460 non-null    object \n",
      " 74  Misc Feature     81 non-null     object \n",
      " 75  Misc Val         2344 non-null   int64  \n",
      " 76  Mo Sold          2344 non-null   int64  \n",
      " 77  Yr Sold          2344 non-null   int64  \n",
      " 78  Sale Type        2344 non-null   object \n",
      " 79  Sale Condition   2344 non-null   object \n",
      " 80  SalePrice        2344 non-null   int64  \n",
      "dtypes: float64(11), int64(27), object(43)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# zum Überblick über Datentypen und fehlende Werte\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data.SalePrice.values\n",
    "X_train = train_data.drop(columns=[\"PID\", \"SalePrice\"])\n",
    "\n",
    "y_test = test_data.SalePrice.values\n",
    "X_test = test_data.drop(columns=[\"PID\", \"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alley', 'Fireplace Qu', 'Pool QC', 'Fence', 'Misc Feature']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anteil der nan-Werte für jede Spalte\n",
    "proportion_nans = X_train.isna().sum() / len(X_train)\n",
    "\n",
    "# Thresholding und Löschen der entsprechenden Spalten\n",
    "\n",
    "# registriere alle Spalten mit mehr als 40% fehlenden Werten\n",
    "threshold = 0.4\n",
    "nan_columns = list(X_train.columns[proportion_nans > 0.4])\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BsmtFin SF 1',\n",
       " 'Overall Qual',\n",
       " 'Total Bsmt SF',\n",
       " 'Lot Frontage',\n",
       " 'Bedroom AbvGr',\n",
       " 'Kitchen AbvGr',\n",
       " '2nd Flr SF',\n",
       " 'Fireplaces',\n",
       " 'Year Built',\n",
       " 'Bsmt Unf SF',\n",
       " 'Wood Deck SF',\n",
       " 'Misc Val',\n",
       " 'Year Remod/Add',\n",
       " 'TotRms AbvGrd',\n",
       " 'BsmtFin SF 2',\n",
       " 'Overall Cond',\n",
       " 'Garage Area',\n",
       " '1st Flr SF',\n",
       " 'Bsmt Full Bath',\n",
       " 'Yr Sold',\n",
       " 'Half Bath',\n",
       " 'Gr Liv Area',\n",
       " 'Bsmt Half Bath',\n",
       " 'Open Porch SF',\n",
       " '3Ssn Porch',\n",
       " 'Garage Yr Blt',\n",
       " 'Screen Porch',\n",
       " 'Mas Vnr Area',\n",
       " 'Lot Area',\n",
       " 'Pool Area',\n",
       " 'Full Bath',\n",
       " 'Garage Cars',\n",
       " 'Low Qual Fin SF',\n",
       " 'Enclosed Porch',\n",
       " 'MS SubClass',\n",
       " 'Mo Sold']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manuelles Erschließen der kategorischen/oridinalen Variablen\n",
    "ordinal_columns = [\"Lot Shape\", \"Land Slope\"]\n",
    "nominal_columns = [\"Street\", \"Lot Config\"]\n",
    "\n",
    "# np.number is ein Oberbegriff für int/float etc.\n",
    "continuous_columns = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Mengenoperationen zum Herauslöschen von Spalten\n",
    "continuous_columns = list(set(continuous_columns).difference(nan_columns))\n",
    "continuous_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lot Shape', 'Land Slope']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinal_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Street', 'Lot Config']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nominal_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformieren zu kategorischen Daten\n",
    "X_train.loc[:, ordinal_columns] = X_train.loc[:, ordinal_columns].astype(\"category\")\n",
    "X_train.loc[:, nominal_columns] = X_train.loc[:, nominal_columns].astype(\"category\")\n",
    "\n",
    "# Theoretisch sollte dies auch mit den Testdaten passieren. Allerdings\n",
    "# ist es durch die späteren Scikit-Learn-Klassen optional\n",
    "X_test.loc[:, ordinal_columns] = X_test.loc[:, ordinal_columns].astype(\"category\")\n",
    "X_test.loc[:, nominal_columns] = X_test.loc[:, nominal_columns].astype(\"category\")\n",
    "\n",
    "for column in ordinal_columns:\n",
    "    # Zugriff auf die jeweiligen Kategorien über pd.Series.values.categories\n",
    "    print(X_train[column].values.categories)\n",
    "    \n",
    "for column in nominal_columns:\n",
    "    # Zugriff auf die jeweiligen Kategorien über pd.Series.values.categories\n",
    "    print(X_train[column].values.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinale Kategorien sollten in der richtigen Weise sortiert sein\n",
    "ordinal_categories = [\n",
    "    [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n",
    "    [\"Gtl\", \"Mod\", \"Sev\"]    \n",
    "]\n",
    "\n",
    "# Nominale Kategorien können aus den Daten geschlossen werden\n",
    "nominal_categories = [\n",
    "    list(X_train[column].values.categories) for column in nominal_columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scikit-Learn Imputation\n",
    "ordinal_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "nominal_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "continuous_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Scikit-Learn Feature Encoding\n",
    "ordinal_encoder = OrdinalEncoder(categories=ordinal_categories)\n",
    "nominal_encoder = OneHotEncoder(categories=nominal_categories, sparse=False)\n",
    "\n",
    "# Scikit-Learn Standardisierung\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Die Vorarbeitung für ordinale Variablen sollte zunächst eine most-frequent-Imputation\n",
    "# ausführen, bevor das feature encoding stattfindet\n",
    "ordinal_preprocessing = make_pipeline(\n",
    "    ordinal_imputer,\n",
    "    ordinal_encoder\n",
    ")\n",
    "\n",
    "# Die Vorarbeitung für nominale Variablen sollte zunächst eine most-frequent-Imputation\n",
    "# ausführen, bevor das feature encoding stattfindet\n",
    "nominal_preprocessing = make_pipeline(\n",
    "    nominal_imputer,\n",
    "    nominal_encoder\n",
    ")\n",
    "\n",
    "\n",
    "# Die Reihenfolge der Vorarbeitung ist hier nicht zwingend, solange\n",
    "# SimpleImputer mit fehlenden Werten umgehen kann\n",
    "continuous_preprocessing = make_pipeline(\n",
    "    continuous_imputer,\n",
    "    scaler\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"ordinal_preprocessor\", ordinal_preprocessing, ordinal_columns),\n",
    "    (\"nominal_preprocessor\", nominal_preprocessing, nominal_columns),\n",
    "    (\"continuous_preprocessor\", continuous_preprocessing, continuous_columns),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Manuelles Transformieren\n",
    "\n",
    "Wir gewinnen einen Überblick über die einzelnen Bestandteile des Preprocessings.\n",
    "Dazu benutzen wir nur die Trainingsdaten. Am Ende sollten alle Transformationen (aber nicht das Fitting!)\n",
    "auch auf die Testdaten angwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "X_ordinal = X_train.loc[:, ordinal_columns]\n",
    "X_nominal = X_train.loc[:, nominal_columns]\n",
    "X_continuous = X_train.loc[:, continuous_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['IR1', 'Mod'],\n",
       "       ['Reg', 'Mod'],\n",
       "       ['Reg', 'Gtl'],\n",
       "       ...,\n",
       "       ['IR1', 'Gtl'],\n",
       "       ['IR1', 'Gtl'],\n",
       "       ['Reg', 'Gtl']], dtype=object)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinal_imputer.fit(X_ordinal)\n",
    "\n",
    "X_ordinal_imputed = ordinal_imputer.transform(X_ordinal)\n",
    "\n",
    "# dies sollte keine nans mehr enthalten\n",
    "X_ordinal_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Pave', 'Inside'],\n",
       "       ['Pave', 'Inside'],\n",
       "       ['Pave', 'Inside'],\n",
       "       ...,\n",
       "       ['Pave', 'Corner'],\n",
       "       ['Pave', 'Inside'],\n",
       "       ['Pave', 'Corner']], dtype=object)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nominal_imputer.fit(X_nominal)\n",
    "\n",
    "X_nominal_imputed = ordinal_imputer.transform(X_nominal)\n",
    "\n",
    "# dies sollte keine nans mehr enthalten\n",
    "X_nominal_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 0.000e+00, 1.600e+03, ..., 1.000e+00, 1.200e+02,\n",
       "        2.008e+03],\n",
       "       [5.220e+02, 0.000e+00, 1.240e+03, ..., 1.000e+00, 3.320e+02,\n",
       "        1.998e+03],\n",
       "       [0.000e+00, 6.770e+02, 1.440e+03, ..., 1.000e+00, 5.360e+02,\n",
       "        2.000e+03],\n",
       "       ...,\n",
       "       [0.000e+00, 0.000e+00, 1.905e+03, ..., 1.000e+00, 1.905e+03,\n",
       "        2.006e+03],\n",
       "       [0.000e+00, 0.000e+00, 1.064e+03, ..., 1.000e+00, 2.850e+02,\n",
       "        1.967e+03],\n",
       "       [4.680e+02, 0.000e+00, 8.820e+02, ..., 1.000e+00, 2.760e+02,\n",
       "        1.972e+03]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_imputer.fit(X_continuous)\n",
    "\n",
    "X_continuous_imputed = continuous_imputer.transform(X_continuous)\n",
    "\n",
    "# dies sollte keine nans mehr enthalten\n",
    "X_continuous_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinal_encoder.fit(X_ordinal_imputed)\n",
    "\n",
    "X_ordinal_imputed_encoded = ordinal_encoder.transform(X_ordinal_imputed)\n",
    "\n",
    "# dies sollte weiterhin 2 Spalten enthalten\n",
    "X_ordinal_imputed_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2344x7 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4688 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nominal_encoder.fit(X_nominal_imputed)\n",
    "\n",
    "X_nominal_imputed_encoded = nominal_encoder.transform(X_nominal_imputed)\n",
    "\n",
    "# dies sollte zusätzliche Spalten enthalten\n",
    "# (es handelt sich hier eventuell um eine \"sparse matrix\" falls sparse=True gesetzt wurde)\n",
    "X_nominal_imputed_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_continuous_imputed)\n",
    "\n",
    "X_continuous_imputed_scaled = scaler.transform(X_continuous_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2344, 2)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ordinal_imputed_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2344, 7)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nominal_imputed_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2344, 36)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_continuous_imputed_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = np.concatenate((\n",
    "    X_ordinal_imputed_encoded,\n",
    "    X_nominal_imputed_encoded.todense(),\n",
    "    X_continuous_imputed_scaled\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2344, 45)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('ordinal_preprocessor',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(add_indicator=False,\n",
       "                                                                copy=True,\n",
       "                                                                fill_value=None,\n",
       "                                                                missing_values=nan,\n",
       "                                                                strategy='most_frequent',\n",
       "                                                                verbose=0)),\n",
       "                                                 ('ordinalencoder',\n",
       "                                                  OrdinalEncoder(categories=[['Reg',\n",
       "                                                                              'IR1',...\n",
       "                                  'Garage Yr Blt', 'Overall Cond', 'Full Bath',\n",
       "                                  'Year Built', 'MS SubClass', 'Mo Sold',\n",
       "                                  'Garage Cars', 'Bsmt Half Bath',\n",
       "                                  'Bedroom AbvGr', 'TotRms AbvGrd',\n",
       "                                  'Low Qual Fin SF', 'Overall Qual',\n",
       "                                  'Garage Area', 'Open Porch SF',\n",
       "                                  'Mas Vnr Area', '1st Flr SF', 'Wood Deck SF',\n",
       "                                  'Screen Porch', 'Yr Sold', 'Misc Val',\n",
       "                                  '3Ssn Porch', 'Pool Area', 'Half Bath',\n",
       "                                  'BsmtFin SF 1', 'Gr Liv Area', 'Lot Area',\n",
       "                                  'Lot Frontage', ...])],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# die Vorverarbeitung kann auch komplett gefittet werden.\n",
    "# Dies beinhaltet\n",
    "#   - Das Finden des most-frequent Wertes für alle ordinalen Variablen auf Grundlage der Trainingsdaten\n",
    "#   - Das Finden des most-frequent Wertes für alle nominalen Variablen auf Grundlage der Trainingsdaten\n",
    "#   - Das Encoding sowohl für ordinale Variablen als auch nominale Variablen\n",
    "#   - Das Standardisieren der metrischen Variablen\n",
    "\n",
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Das Machine Learning Modell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Lineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('ordinal_preprocessor',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='most_frequent',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('ordinal...\n",
       "                                                   'Overall Qual',\n",
       "                                                   'Garage Area',\n",
       "                                                   'Open Porch SF',\n",
       "                                                   'Mas Vnr Area', '1st Flr SF',\n",
       "                                                   'Wood Deck SF',\n",
       "                                                   'Screen Porch', 'Yr Sold',\n",
       "                                                   'Misc Val', '3Ssn Porch',\n",
       "                                                   'Pool Area', 'Half Bath',\n",
       "                                                   'BsmtFin SF 1',\n",
       "                                                   'Gr Liv Area', 'Lot Area',\n",
       "                                                   'Lot Frontage', ...])],\n",
       "                                   verbose=False)),\n",
       "                ('ridge',\n",
       "                 Ridge(alpha=10.0, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='auto', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "\n",
    "regressor = Ridge(alpha=10.0, fit_intercept=True)\n",
    "\n",
    "baseline_model = make_pipeline(\n",
    "    preprocessor,\n",
    "    regressor\n",
    ")\n",
    "\n",
    "# Ja, es ist wirklich so einfach\n",
    "baseline_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412686015082297"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring der Trainingsdaten - r²\n",
    "baseline_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('ordinal_preprocessor',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='most_frequent',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('ordinal...\n",
       "                                           init=None, learning_rate=0.1,\n",
       "                                           loss='ls', max_depth=3,\n",
       "                                           max_features=None,\n",
       "                                           max_leaf_nodes=None,\n",
       "                                           min_impurity_decrease=0.0,\n",
       "                                           min_impurity_split=None,\n",
       "                                           min_samples_leaf=1,\n",
       "                                           min_samples_split=2,\n",
       "                                           min_weight_fraction_leaf=0.0,\n",
       "                                           n_estimators=100,\n",
       "                                           n_iter_no_change=None,\n",
       "                                           presort='auto', random_state=None,\n",
       "                                           subsample=1.0, tol=0.0001,\n",
       "                                           validation_fraction=0.1, verbose=0,\n",
       "                                           warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "boosting = GradientBoostingRegressor(n_estimators=100)\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    boosting\n",
    ")\n",
    "\n",
    "# Ja, es ist wirklich so einfach\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9579008662857603"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring der Trainingsdaten - r²\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('columntransformer',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('ordinal_preprocessor',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('simpleimputer',\n",
       "                                                                                          SimpleImputer(add_indicator=False,\n",
       "                                                                                                        copy=True,\n",
       "                                                                                                        fill_value=None,\n",
       "                                                                                                        missi...\n",
       "                                                                          'Frontage', ...])],\n",
       "                                                          verbose=False)),\n",
       "                                       ('ridge',\n",
       "                                        Ridge(alpha=10.0, copy_X=True,\n",
       "                                              fit_intercept=True, max_iter=None,\n",
       "                                              normalize=False,\n",
       "                                              random_state=None, solver='auto',\n",
       "                                              tol=0.001))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'ridge__alpha': [0.0001, 0.001, 0.01, 1.0, 10.0,\n",
       "                                          100.0]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"ridge__alpha\": [1e-4, 1e-3, 1e-2, 1.0, 10.0, 100.0]}\n",
    "\n",
    "\n",
    "hp_tuned_model = GridSearchCV(\n",
    "    baseline_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "hp_tuned_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8404173794044503"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_tuned_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8147468212224674"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring der Testdaten - r²\n",
    "baseline_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8189757328232418"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring der Testdaten - r²\n",
    "hp_tuned_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8896564953477966"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring der Testdaten - r²\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
