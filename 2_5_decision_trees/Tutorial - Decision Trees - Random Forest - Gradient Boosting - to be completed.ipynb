{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00002-c2aabb42-89b1-4a7a-bf70-652f9b2c64c6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1611833332447,
    "source_hash": "aa3ac375",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Importiere Decision Tree aus Scikit-Learn\n",
    "# TODO: Importiere Random Forest aus Scikit-Learn\n",
    "# TODO: Importiere Gradient Boosting aus Scikit-Learn\n",
    "# TODO: Importiere Evaluationsmetriken\n",
    "# TODO: Visualisierungstools\n",
    "\n",
    "# TODO: Importieren von `train_test_split` aus Scikit-Learn\n",
    "\n",
    "import utils_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-a5b98850-8886-487f-8840-383b29627dab",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## 1. Decision Trees\n",
    "\n",
    "Decision Trees sind überwachte Lernmodelle, die sich sowohl für Klassifikationsprobleme (die Zielvariable ist ein diskreter/kategorieller Wert) als auch für Regressionsprobleme (die Zielvariable ist ein metrischer Wert) eignen.\n",
    "\n",
    "Wir benutzen für dieses Tutorial die in Scikit-Learn implementierten Klassen für Decision Trees: `DecisionTreeClassifier` und `DecisionTreeRegressor`. Achten Sie in jedem Fall darauf, die für das Problem passende Klasse zu verwenden.\n",
    "\n",
    "Nach dem Import der Klassen reicht ein Blick in den Docstring der Klassen, das heißt durch\n",
    "\n",
    "```python\n",
    ">>> DecisionTreeClassifier?\n",
    "```\n",
    "\n",
    "um zu erkennen, dass bei der Initialisierung des Decision Trees viele Argumente übergeben werden können. Wir beschränken uns hier nur auf das wichtigste davon, nämlich `max_depth`.\n",
    "\n",
    "Weitere relevante Funktionen und Attribute sind\n",
    "- `fit`\n",
    "- `predict`\n",
    "\n",
    "sowie fortgeschrittene Attribute:\n",
    "- `predict_proba`\n",
    "- `feature_importances_` (erst nach dem Fit verfügbar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-d10ef959-11d0-4d88-9674-5a5f91219927",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 1.1 Decision Trees für Klassifikation\n",
    "\n",
    "Zunächst beschäftigen wir uns mit dem Einlesen und Vorbereiten eines Toy-Datensatzes. Der Datensatz soll den Zusammenhang zwischen zwei Features, die die Genaktivität zweier Gene einer nicht genannten Spezies (Beispiel: Mäuse) und dem Phänotyp dieser Spezies darstellen. Es gibt 4 verschiedene Phänotypen (A, B, C, D) - es handelt sich also um ein Klassifikationsproblem. Decision Trees können auf natürliche Weise solche Multi-Class-Klassifikationen handhaben (tatsächlich können das alle Modelle in Scikit-Learn, aber nur weil intern dazu eine gewisse zusätzliche Logik - genannt One-vs-All - verbaut ist. Decision Trees handhaben Multi-Class ganz selbstständig.)\n",
    "\n",
    "Wir müssen\n",
    "\n",
    "- die Daten einlesen\n",
    "- die Phänotypen A, B, C, D in numerische Werte übersetzen\n",
    "- die Daten in Trainings- und Testdaten aufspalten\n",
    "- die Daten in einem Scatterplot visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-121ee766-fc8f-4981-b4b0-05f1dc7e464b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1611833669943,
    "source_hash": "50005f79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Einlesen der Daten `toy_gene_data.csv`\n",
    "# TODO: Aufspalten der Daten in Trainings- und Testdaten\n",
    "# TODO: Daten verarbeiten\n",
    "# TODO: Visualisieren der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun fitten wir das Modell auf die übliche Weise und evaluieren. Wir nutzen die **Genauigkeit** bzw. *Korrekte-Klassifikations-Rate* als Metrik zum Evaluieren, da wir es mit einem Klassifikationsproblem zu tun haben. Wir können dafür eine Funktion schreiben oder die Funktion `accuracy_score` aus Scikit-Learn importieren.\n",
    "\n",
    "Dabei sollten wir auch das Argument `max_depth` variieren und verstehen, welchen Einfluss dieses auf den Trainingsfehler bzw. Testfehler hat (Overfitting versus Underfitting?).\n",
    "\n",
    "Beim Training des Decision Trees ist folgendes relevant:\n",
    "- der Decision Tree versucht beim Aufspalten der Daten in jedem Knoten den **Gini-Koeffizienten** (alternativ die **Entropie**) in den nachfolgenden Knoten zu minimieren.\n",
    "- die Vorhersage in den Blättern ist die häufigste Klasse unter den Trainingsdatenpunkten, die in diesem Blatt gelandet sind.\n",
    "- das Aufspalten in jedem Knoten ist *greedy*, das heißt es wird so gespalten, wie es zu jedem Zeitpunkt am sinnvollsten erscheint, ohne auf den weiteren Verlauf des Baums zu achten.\n",
    "- die Größe des Baums ist vor allem von `max_depth` abhängig. Daneben gibt es andere Abbruchkriterien für das Wachsen des Baums, die durch weitere Argumente bei der Initialisierung eingestellt werden können. In jedem endet das Wachstum, wenn in einem Blatt alle Trainingsdatenpunkte derselben Klasse angehören.\n",
    "\n",
    "Weiterhin gibt es die Möglichkeit, die Entscheidungen des trainierten Baums zu visualisieren. Dazu existiert eine Funktion `plot_tree` in Scikit-Learn. Als zusätzliche Visualisierung werden wir auch die Entscheidungsgrenzen des Modells veranschaulichen, was wir dank der Benutzung von nur zwei Features problemlos können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00003-121ee766-fc8f-4981-b4b0-05f1dc7e464b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1611833669943,
    "source_hash": "50005f79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Modell trainieren\n",
    "# TODO: Modell evaluieren\n",
    "# TODO: Modell visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-821fcffe-7ba0-42c1-89fb-041f868d3ad8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 1.2 Decision Trees für Regressionen\n",
    "\n",
    "Durch leichte Änderungen das Algorithmus können Decision Trees auch für Regressionsprobleme verwendet werden.\n",
    "Wir untersuchen ein Regressionsproblem, das nur auf einem Feature beruht, damit für wie gehabt den Fit in einem Graphen visualisieren können. Wir entscheiden uns für den Toy-Automobile Datensatz.\n",
    "\n",
    "Folgende Unterschiede sind bei einem Decision Tree für Regressionen im Vergleich zu einem für Klassifikationen zu beachten:\n",
    "- der Regressions-Decision Tree versucht nicht den **Gini-Koeffizienten** (oder alternativ die **Entropie**) der Knoten zu minimieren, sondern minimiert den **Mean-Squared-Error** der Knoten.\n",
    "- die Vorhersage an einem Blatt ist nicht die häufigste Klasse, sondern der durchschnittliche Werte der Zielvariablen aller Trainingsdatenpunkte in diesem Blatt\n",
    "- das Abbruchkriterium für das Wachsen des Baums ist wieder von `max_depth` sowie von weiteren Argumenten abhängig. Außerdem endet das Wachsum in jedem Fall, wenn der **Mean-Squared-Error** in einem Knoten auf Null sinkt, wenn also alle dortigen Trainingsdatenpunkte denselben Wert der Zielvariablen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-3f83ba20-a482-4199-9fbd-ca21a34b476e",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Einlesen der Daten `toy_auto_data_A.csv` und `toy_auto_data_test.csv`\n",
    "# TODO: Visualisieren der Daten\n",
    "# TODO: Modell trainieren\n",
    "# TODO: Modell evaluieren\n",
    "# TODO: Modell visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-bfdb05a2-4c15-4729-8f6e-dd823d991d5e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## 2. Random Forests\n",
    "\n",
    "Random Forests lösen das wesentliche Problem von Decision Trees: die Tendenz, extremes Overfitting zu betreiben. Dazu besteht der Random Forest aus einer Menge - einem *Ensemble* - von Decision Trees, die sich alle leicht voneinander unterscheiden. Der Vorgang wird auch **Bagging** genannt. Die Decision Trees im *Ensemble* unterscheiden sich dadurch, dass sie jeweils einen leicht anderen Teil der Trainingsdaten kennengelernt haben.\n",
    "\n",
    "Diese Randomisierung der Trainingsdaten kann auf verschiedene Weise erfolgen:\n",
    "- zufällige Auswahl von 50-80% der Trainingsdaten für jeden Baum. Überschneidungen zwischen den einzelnen Randomisierungen sind natürlich möglich.\n",
    "- zufälliges Ziehen-mit-Zurücklegen der Trainingdatenpunkte. Dies wird auch **Bootstrapping** genannt. Es wird einfach wiederholt ein Trainingsdatenpunkt aus den gesamten Trainingsdaten gezogen, dann aber wieder \"zurückgelegt\", sodass ein Trainingsdatenpunkt in dem neuen, randomisierten Trainingsdatensatz auch mehrmals vorkommen kann.\n",
    "- Kombinationen aus den beiden vorher genannten Strategien\n",
    "\n",
    "Zusätzlich kann man für jeden randomisierten Trainingsdatensatz auch eine zufällige Auswahl der Features vornehmen.\n",
    "\n",
    "Die Argumente bei der Initialisierung des Random Forest erlauben es, alle diese Randomisierungen selbstständig einzustellen. Im einfachsten Fall sollte man es allerdings bei den Default-Werten belassen.\n",
    "\n",
    "Die wichtigsten Argumente für uns sind deshalb\n",
    "- `max_depth`\n",
    "- `n_estimators`\n",
    "\n",
    "Typische Größen für `n_estimators` sind 100, 200, 500 oder maximal 1000. Danach haben zusätzliche Bäume meist keinen Effekt mehr. `max_depth` muss als Hyperparameter manchmal getunt werden.\n",
    "\n",
    "Zusätzlich brauchen wir\n",
    "- `fit`\n",
    "- `predict`\n",
    "\n",
    "sowie eventuell fortgeschrittene Attribute:\n",
    "- `predict_proba`\n",
    "- `feature_importances_` (erst nach dem Fit verfügbar)\n",
    "\n",
    "Wichtig ist, dass wir von dem Training der einzelnen Decision Trees nichts mitbekommen, da dies im Aufruf der `fit` und `predict` Funktionen des Random Forest intern geschieht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-05e51525-0c32-471e-b441-445d3447c43d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 2.1 Random Forests für Klassifikationen\n",
    "\n",
    "Wir vergleichen das Ergebnis des Fits zu dem eines einzelnen Decision Trees und versuchen zu verstehen, wie der Random Forest gegen Overfitting arbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00015-04e0261e-8fb8-4529-800d-6f9747569640",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 169,
    "execution_start": 1611832455113,
    "source_hash": "8bb024a1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Einlesen der Daten `toy_gene_data.csv`\n",
    "# TODO: Aufspalten der Daten in Trainings- und Testdaten\n",
    "# TODO: Daten verarbeiten\n",
    "# TODO: Visualisieren der Daten\n",
    "# TODO: Modell trainieren\n",
    "# TODO: Modell evaluieren\n",
    "# TODO: Modell visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-a8469e89-4ecd-489a-9923-8eeeb7aee0cd",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 2.2. Random für Regressionen\n",
    "\n",
    "Auch für Regressionsprobleme eignet sich ein Random Forest. Wir vergleichen den Fit auch hier mit dem eines einzelnen Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Einlesen der Daten `toy_auto_data_A.csv` und `toy_auto_data_test.csv`\n",
    "# TODO: Visualisieren der Daten\n",
    "# TODO: Modell trainieren\n",
    "# TODO: Modell evaluieren\n",
    "# TODO: Modell visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting\n",
    "\n",
    "Gradient Boosting ist die zweite der Erweiterungen des Decision Trees. Sie beruht auf der Idee des **Boostings**: die Kombination vieler einfacher Modelle - auch genannt *weak learners* - zu einem starken Modell. Dies geschieht über die sequentielle Fehlerkorrektur. Decision Trees eignen sich sehr gut als *weak learners* weil sie schnell zu trainieren sind. Typischerweise hat jeder Decision Tree im **Boosting** eine sehr kleine Tiefe von 3-5. Manchmal sogar nur die Tiefe 1.\n",
    "\n",
    "Das Gradient Boosting hat ingesamt drei sehr wichtige und sensitive Hyperparameter:\n",
    "- `n_estimators`\n",
    "- `max_depth`\n",
    "- `learning_rate`\n",
    "\n",
    "Eventuell müssen alle diese Hyperparamter getunt werden.\n",
    "\n",
    "Den Effekt der Hyperparameter kann mit in folgender interaktiver Graphik verstehen lernen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bd9eeafd8941ae9f204cc3df380f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff32a740d4464d688e605c73942a6f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b3d61892bf415499b42d16cb0b1240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(IntSlider(value=1, continuous_update=False, description='# Iterations', min=1), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import utils_boosting\n",
    "\n",
    "X_train, y_train = utils_boosting.generate_data(n_samples=50, random_state=2)\n",
    "X_test, y_test = utils_boosting.generate_data(n_samples=200, random_state=5)\n",
    "\n",
    "interactive_plot, ui = utils_boosting.get_interactive_boosting(\n",
    "    X_train, y_train, X_test, y_test, max_depth=3)\n",
    "display(interactive_plot, ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Gradient Boosting für Regressionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Gradient Boosting für Klassifikationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9415d402-616f-400d-90bc-042de9dd48d9",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
