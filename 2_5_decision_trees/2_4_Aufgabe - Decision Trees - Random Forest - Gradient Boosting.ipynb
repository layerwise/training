{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ziel dieser Aufgabe ist es, den **Decision Tree** auf verschiedene Weise zu erweitern: (1) durch *Bagging*, in Form eines **Random Forest** und (2) durch *Boosting*. Wir benutzen zur Implementierung des **Decision Trees** die in Scikit-Learn verfügbare Implementierung und erweitern diese für die gewünschte Implementierung der zwei Algorithmen.\n",
    "\n",
    "## (3.1.1) Decision Trees <span style=\"color:green; font-size:1em\">(o)</span> <span style=\"font-size:1em\">&#x1F4D7;</span>\n",
    "\n",
    "**<span style=\"color:green; font-size:2em\">(a)</span>** <span style=\"font-size:2em\">&#x1F4D7;</span> Laden Sie den in Scikit-Learn verfügbaren Iris-Datensatz und behalten Sie von den vier Features des Datensatzes nur die ersten zwei, damit das Problem einfach zu visualisieren ist. Plotten Sie den Datensatz mit `plt.scatter` und weisen Sie den Klassen verschiedene Farben zu.\n",
    "\n",
    "**<span style=\"color:green; font-size:2em\">(b)</span>** <span style=\"font-size:2em\">&#x1F4D7;</span> In der begleitenden Python-File `utils_viz.py` befindet sich eine Funktion `visualize_classifier`, denen als Argumente ein Scikit-Learn-Modell sowie die Trainings-Beobachtungen und Trainings-Labels übergeben werden können. Machen Sie sich mit der Funktion vertraut.\n",
    "\n",
    "**<span style=\"color:green; font-size:2em\">(c)</span>** <span style=\"font-size:2em\">&#x1F4D7;</span> Trainieren Sie einen einfachen **Decision Tree**. Sie sollten bis auf das Argument `max_depth` alle Argumente bei ihren Default-Werten belassen. Varieren Sie `max_depth` und benutzen Sie die Funktion aus **b)** um die Entscheidungsgrenzen Bäume verschiedener Tiefe zu visualisieren. Trainieren Sie auf der gesamten Datenmenge.\n",
    "\n",
    "**<span style=\"color:green; font-size:2em\">(d)</span>** <span style=\"font-size:2em\">&#x1F4D7;</span> Teilen Sie nun die Daten in Trainings- und Testdaten auf. Weisen Sie zufällig 50% der Daten dem Trainingsdatensatz zu und die verbliebenen Daten dem Testdatensatz (Tipp: benutzen Sie am besten die Funktion `train_test_split`, die unten beispielhaft erklärt wird). Trainieren Sie nun für jede Tiefe von $1-5$ einen **Decision Tree** und evaluieren Sie mit den Testdaten. Plotten Sie die Korrekte-Klassifikationsrate/Genauigkeit gegen die Tiefe des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "# optional\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz - Dokumentation\n",
    "iris = load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.4.2) Bagging - Random Forests - Classification <span style=\"color:orange; font-size:1em\">(oo) </span> <span style=\"font-size:1em\">&#x1F4D9;</span>\n",
    "\n",
    "Unser nächstes Ziel ist es, die Entscheidungsgrenze eines **Random Forest** mit dem eines **Decision Trees** zu vergleichen. Wir betrachten einen **Random Forest**, der aus 100 Bäumen zusammengesetzt ist. Jeder dieser Bäume soll auf nur auf einem Teil der Trainingsdaten trainiert werden. Wählen Sie für jeden Baum zufällig 50% der Trainingsdaten aus (Tipp: benutzen Sie zum Beispiel `np.random.permutation` oder `np.random.choice` oder `train_test_split`). Die Vorhersage-Funktion soll ein einfacher *Majority-Vote* der einzelnen Bäume sein (die Vorhersage ist die Klasse, die von den meisten Bäumen vorhergesagt wird).\n",
    "\n",
    "**<span style=\"color:green; font-size:2em\">(a) </span>** <span style=\"font-size:2em\">&#x1F4D9;</span> Implementieren Sie die unten aufgeführte Funktion `fit`, die als Input einen Random Forest erhält und den Fit der einzelnen Bäume ausführt.\n",
    "\n",
    "**<span style=\"color:orange; font-size:2em\">(b) </span>** <span style=\"font-size:2em\">&#x1F4D9;</span> Implementieren Sie die unten aufgeführte Funktion `predict`, die als Input einen Random Forest erhält und die Vorhersagen der einzelnen Bäume ermittelt und daraufhin einen *Majority-Vote* ausführt (Tipp: `scipy.stats.mode`).\n",
    "\n",
    "**<span style=\"color:red; font-size:2em\">(c) </span>** <span style=\"font-size:2em\">&#x1F4D9;</span> In dieser Aufgabe soll der Random Forest objekt-orientiert implementiert werden. Implementieren Sie dazu die entsprechenden Funktionen der unten bereits vorimplementierten Klasse, sodass ihr **Random Forest** Klassifikator mit `visualize_classifier` aus `utils_viz.py` benutzt werden kann (Tipp: `scipy.stats.mode`).\n",
    "\n",
    "**<span style=\"color:green; font-size:2em\">(d) </span>** <span style=\"font-size:2em\">&#x1F4D9;</span> Teilen Sie die Daten wie zuvor in Trainings- und Testdaten auf. Trainieren Sie das Modell und visualisieren Sie die Entscheidungsgrenze. Berechnen Sie die Test-Klassifikationsrate und vergleichen Sie mit den **Decision Trees** aus der vorherigen Aufgabe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting eines Random Forest\n",
    "\n",
    "# Random Forest ist ein Ensemble aus Bäumen - darstellbar zum Beispiel als List\n",
    "random_forest = []\n",
    "for i in range(100):\n",
    "    new_tree = DecisionTreeClassifier(max_depth=5)\n",
    "    random_forest.append(new_tree)\n",
    "\n",
    "# Alternative\n",
    "# random_forest = [DecisionTreeClassifier(max_depth=5) for _ in range(100)]\n",
    "\n",
    "\n",
    "# TODO\n",
    "def fit(random_forest, X, y):\n",
    "    \n",
    "    for tree in random_forest:\n",
    "        # ...\n",
    "        pass\n",
    "        \n",
    "    \n",
    "# Fit aufrufen        \n",
    "fit(random_forest, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def predict(random_forest, X):\n",
    "    \n",
    "    # Zum Speichern der Vorhersagen\n",
    "    y_preds = []\n",
    "    \n",
    "    for tree in random_forest:\n",
    "        # ...\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorimplementierte Klasse:\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Die Funktion `__init__` wird automatisch aufgerufen, wenn folgender Code ausgeführt wird:\n",
    "        \n",
    "        >>> model = RandomForestClassifier()\n",
    "        \n",
    "        Danach speichert das Modell einfach 100 zufällige generierte Decision Trees,\n",
    "        die mit der Scikit-Learn Implementierung erstellt wurden. Durch Ausführen des folgenden\n",
    "        Codes können die Bäume, die im Modell gespeichert sind, inspiziert werden:\n",
    "        >>> print(model.trees)\n",
    "        \"\"\"\n",
    "        self.trees = [DecisionTreeClassifier(max_depth=5) for _ in range(100)]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Die Funktion `fit` soll genauso funktionieren, wie für Scikit-Learn üblich:\n",
    "        >>> model.fit(X_train, y_train)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Die Funktion `predict` soll genauso funktionieren, wie für Scikit-Learn üblich:\n",
    "        >>> y_pred = model.predict(X)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.4.3) Bagging - Random Forests - Regression <span style=\"color:orange; font-size:1em\">(oo) </span> <span style=\"font-size:1em\">&#x1F4D9;</span>\n",
    "\n",
    "In dieser Aufgabe soll wie bei der Klassifikation ein einfacher **Random Forest** Klassifikator erstellt werden, der auf der Basis-Klasse `DecisionTreeRegressor` beruht. Anstatt eines *Majority-Votes* ist hier der Vorhersage-Mechanismus ein einfaches *Averaging* der Vorhersagen der einzelnen Bäume. Jeder einzelne Baum soll wie zuvor auf 50% der Trainingsdaten trainiert werden.\n",
    "\n",
    "**<span style=\"color:green; font-size:2em\">(a) </span>** <span style=\"font-size:2em\">&#x1F4D9;</span> Implementieren Sie eine Funktion `fit`, die als Input einen Random Forest erhält und den Fit der einzelnen Bäume ausführt. Orientieren Sie sich an Aufgabe **2a)**.\n",
    "\n",
    "**<span style=\"color:orange; font-size:2em\">(b) </span>** <span style=\"font-size:2em\">&#x1F4D9;</span> Implementieren Sie eine Funktion `predict`, die als Input einen Random Forest erhält und die Vorhersagen der einzelnen Bäume ermittelt und daraufhin ein *Averaging* ausführt. Orientieren Sie sich an Aufgabe **2b)**.\n",
    "\n",
    "**<span style=\"color:red; font-size:2em\">(c) </span>** <span style=\"font-size:2em\">&#x1F4D9;</span> In dieser Aufgabe soll der Random Forest Regressor objekt-orientiert implementiert werden. Implementieren Sie dazu die entsprechenden Funktionen der unten bereits vorimplementierten Klasse.\n",
    "\n",
    "**<span style=\"color:orange; font-size:2em\">(d) </span>** <span style=\"font-size:2em\">&#x1F4D9;</span> Die Funktion `utils_tree.benchmark` gibt die durchschnittliche Performance des Klassifikators für 100 zufällige Train/Test-Splits aus. Benutzen Sie diese Funktion, um für zwei Beispiel-Datensätze (`load_boston` und `load_diabetes` aus Scikit-Learn) und für eine variable Menge an Bäumen, das heißt für `nb_trees = [1,2,4,8,16,32]`, die Performance zu berechnen. Die Performance wird statt wie üblich als quadratischer Fehler als *Score* berechnet, der zwischen $-\\infty$ und $1$ liegt und die Qualität der Regression beschreibt. Ein Score von $1$ steht dabei für eine perfekte Regression, während ein Score von $0$ äquivalent zu einem konstanten Modell ist. Ein negativer Score steht für einen Regressor, der systematisch falsch liegt. Plotten Sie die Ergebnisse. Vergleichen Sie mit dem Ergebnis für einen einfachen Baum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from utils_tree import benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indem die Klasse von RegressorMixin erbt enthält sie sofort die Funktion `score`,\n",
    "# sodass diese nicht implementiert werden muss.\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Vorimplementierte Klasse:\n",
    "class RandomForestRegressor(RegressorMixin):\n",
    "    def __init__(self, max_depth=5, nb_trees=10):\n",
    "        \"\"\"\n",
    "        Die Funktion `__init__` wird automatisch aufgerufen, wenn folgender Code ausgeführt wird:\n",
    "        \n",
    "        >>> model = RandomForestRegressor()\n",
    "        \n",
    "        Danach speichert das Modell einfach 100 zufällige generierte Decision Trees,\n",
    "        die mit der Scikit-Learn Implementierung erstellt wurden. Durch Ausführen des folgenden\n",
    "        Codes können die Bäume, die im Modell gespeichert sind, inspiziert werden:\n",
    "        >>> print(model.trees)\n",
    "        \"\"\"\n",
    "        self.trees = [DecisionTreeRegressor(max_depth=max_depth) for _ in range(nb_trees)]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Die Funktion `fit` soll genauso funktionieren, wie für Scikit-Learn üblich:\n",
    "        >>> model.fit(X_train, y_train)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Die Funktion `predict` soll genauso funktionieren, wie für Scikit-Learn üblich:\n",
    "        >>> y_pred = model.predict(X)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "diabetes = load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.4.4) Boosting - Regression <span style=\"color:red; font-size:1em\">(ooo) </span> <span style=\"font-size:1em\">&#x1F4D8;</span>\n",
    "\n",
    "In dieser Aufgabe implementieren Sie einen einfachen Boosting-Algorithmus. Wie bei einem Bagging-Algorithmus beruht auch ein Boosting-Algorithmus auf der Vorhersage eines Ensembles verschiedener *weak learners*, in diesem Fall **Decision Trees**. Im Gegensatz zum Bagging werden die Vorhersagen der einzelner Lerner aber nicht unabhängig voneinander vorgenommen und dann zusammengefasst, sondern die Aufgabe jedes Lerners ist es, die Vorhersage aller vorherigen Lerner zu verbessern.\n",
    "\n",
    "Die hier betrachtete Variante des Boostings soll folgendermaßen funktionieren. Sei \n",
    "\n",
    "$$\n",
    "F_k(x^{(i)}) = f_1(x) + f_2(x) + ... + f_k(x)\n",
    "$$\n",
    "\n",
    "die Vorhersage eines Boosting mit bereits $k$ trainierten Bäumen. Ein neuer Lerner wird nun einfach auf dem Residual des Fehlers dieser Vorhersage trainiert. Wir definieren für jeden Datenpunkt eine neue Zielvariable:\n",
    "\n",
    "$$\n",
    "y_{k+1}^{(i)} = r(x^{(i)}) = y_T^{(i)} - F_k(x^{(i)})\n",
    "$$\n",
    "\n",
    "und trainieren den neuen Baum nun auf dem Datensatz $\\{(x^{(i)}, y_{k+1}^{(i)}) \\; | \\; i = 1, ..., N\\}$. Die dabei gelernte Vorhersagefunktion sei $f_{k+1}(x)$\n",
    "\n",
    "Die Vorhersagefunktion des initialen Baums sei mit $f_1(x) = \\bar{y} = \\sum_{i=1}^N y_T^{(i)}$ schlicht der durchschnittliche Wert der Zielvariablen der Trainingsdaten.\n",
    "\n",
    "Sobald die gewünschten $K$ Bäume trainiert sind, ist die Vorhersagefunktion des gesamten Modells einfach die Summe der einzelnen Bäume:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = F_K(x) = f_1(x) + f_2(x) + ... + f_K(x)\n",
    "$$\n",
    "\n",
    "\n",
    "**<span style=\"color:red; font-size:2em\">(a) </span>** <span style=\"font-size:2em\">&#x1F4D8;</span> Implementieren Sie den beschriebenen Boosting-Algorithmus, indem Sie die unten vorimplementierte Klasse vervollständigen.\n",
    "\n",
    "**<span style=\"color:orange; font-size:2em\">(d) </span>** <span style=\"font-size:2em\">&#x1F4D8;</span> Benchmarking: Wiederholen Sie die Experimente aus Aufgabe **3b)** für den Boosting-Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Vorimplementierte Klasse:\n",
    "class SimpleBoostedTreeRegressor(RegressorMixin):\n",
    "    def __init__(self, max_depth=5, nb_trees=10):\n",
    "        \"\"\"\n",
    "        Die Funktion `__init__` wird automatisch aufgerufen, wenn folgender Code ausgeführt wird:\n",
    "        \n",
    "        >>> model = SimpleBoostedTreeRegressor()\n",
    "        \n",
    "        Danach speichert das Modell einfach 100 zufällige generierte Decision Trees,\n",
    "        die mit der Scikit-Learn Implementierung erstellt wurden. Durch Ausführen des folgenden\n",
    "        Codes können die Bäume, die im Modell gespeichert sind, inspiziert werden:\n",
    "        >>> print(model.trees)\n",
    "        \"\"\"\n",
    "        self.trees = [DecisionTreeRegressor(max_depth=max_depth) for _ in range(nb_trees)]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Die Funktion `fit` soll genauso funktionieren, wie für Scikit-Learn üblich:\n",
    "        >>> model.fit(X_train, y_train)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Die Funktion `predict` soll genauso funktionieren, wie für Scikit-Learn üblich:\n",
    "        >>> y_pred = model.predict(X)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.4.5) Vergleichen <span style=\"color:green; font-size:1em\">(o) </span> <span style=\"font-size:1em\">&#x1F4D7;</span>\n",
    "\n",
    "Nutzen Sie untenstehenden Code, um die verschiedenen Algorithmen miteinander zu vergleichen, wenn sich jeweils die Komplexität der zu Grunde liegenden Bäume erhöht. Interpretieren Sie das Ergebnis. (Verwenden Sie, falls Sie in den vorherigen Aufgaben den `RandomForestRegressor` und den `SimpleBoostedTreeRegressor` nicht implementiert haben stattdessen die in Scikit-Learn verfügbaren Implementierungen.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "depths     = [1,2,3,4,5,6,7,8]\n",
    "datasets   = [boston,diabetes]\n",
    "names      = ['boston','diabetes']\n",
    "algorithms = [\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    SimpleBoostedTreeRegressor]\n",
    "\n",
    "for dataset,name in zip(datasets,names):\n",
    "    plt.figure()\n",
    "    plt.title(name)\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        \n",
    "        acc = [benchmark(algorithm(max_depth=i),dataset)[1] for i in depths]\n",
    "        plt.plot(depths, acc, 'o-', label=algorithm.__name__)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('tree depth')\n",
    "    plt.ylabel('coefficient of determination')\n",
    "    plt.legend(loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
