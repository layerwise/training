{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.3) Optimieren <span style=\"color:red; font-size:1em\">(ooo)</span> <span style=\"font-size:1em\">&#x1F4D8;</span>\n",
    "\n",
    "In dieser Aufgabe implementieren wir verschiedene Strategien zum Optimieren der Parameter einer linearen Regression auf einem Toy-Datensatz. Wir nutzen dazu diverse Hilfsfunktionen aus der begleitenden Datei `utils_optimization.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "%matplotlib widget\n",
    "# %matplotlib notebook\n",
    "\n",
    "from utils_optimization import x_train, y_train\n",
    "from utils_optimization import visualize_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3.1) Vorbereitung - Random Search\n",
    "\n",
    "In dieser Aufgabe implementieren wir den Algorithmus aus dem Notebook `Optimieren.ipynb` Abschnitt **2.1** und lernen dabei die Struktur der Aufgabe kennen. Die Aufgabe basiert auf einer Hilfsfunktion `visualize_optimization` aus `utils_optimization`, die als erstes Argument eine Funktion erhält. Diese Funktion sollte folgende Struktur haben:\n",
    "\n",
    "```python\n",
    "def update_function(x_train, y_train, w, bias, **kwargs):\n",
    "    # do something with x_train, y_train\n",
    "    \n",
    "    # compute the variables\n",
    "    # - w_new\n",
    "    # - bias_new\n",
    "    \n",
    "    # optionally use more kwargs    \n",
    "    return w_new, bias_new\n",
    "\n",
    "\n",
    "# Then visualize\n",
    "fig, axes = visualize_optimization(update_function)\n",
    "```\n",
    "\n",
    "Das heißt, die Funktion erhält einen Trainingsdatensatz (`x_train` und `y_train`), die bisherigen Werte der Parameter (`w`, `bias`) und optional weitere Argumente. Daraus sollen neue Werte `w_new` und `bias_new` errechnet werden. Ihnen sind alle Freiheiten gegeben, doch unser Ziel sollte sein, zum Schluss hier den Gradientenabstieg zu implementieren.\n",
    "\n",
    "Danach wird die Funktion als Argument (ja, das geht in Python!) der Funktion `visualize_optimization` übergeben.\n",
    "\n",
    "Zunächst fangen wir aber mit etwas einfacherem an. Führen Sie den untestehende Code aus und interpretieren Sie das Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Funktion `random_search_simple` erhält ebenfalls alle geforderten Argumente\n",
    "# - nutzt diese aber nicht. Stattdessen wird ein zufälliger Wert für `w_new` und `bias_new` \n",
    "# erzeugt und zurückgegeben.\n",
    "def random_search_simple(x_train, y_train, w, bias):\n",
    "    w_new = np.random.uniform(-3, 3)\n",
    "    bias_new = np.random.uniform(-4, 4)\n",
    "    \n",
    "    return w_new, bias_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c4def25bf642bdb2465b73bf399133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wir nutzen `random_search_simple` nun als Argument von `visualize_optimization`\n",
    "fig, axes = visualize_optimization(random_search_simple, max_iter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3.2) Random Search adaptieren\n",
    "\n",
    "Versuchen Sie nun Folgendes: Ändern Sie das Argument `max_iter` der Funktion `visualize_optimization`. Was ändert sich?\n",
    "\n",
    "Adaptieren Sie nun die Funktion `random_search_simple` und erstellen Sie eine Funktion `random_search_advanced`, die den Algorithmus aus Abschnitt **2.2** das Notebooks `Optimieren I.ipynb` umsetzt. Visualisieren Sie die Optimierung. Hier der Startpunkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7718b002d94593a8d39de57fc85983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def random_search_advanced(x_train, y_train, w, bias):\n",
    "    current_loss = np.mean((x_train * w + bias - y_train)**2)\n",
    "    \n",
    "    bias_new = bias + np.random.normal(loc=0.0, scale=0.5)\n",
    "    w_new = w + np.random.normal(loc=0.0, scale=0.5)\n",
    "    \n",
    "    new_loss = np.mean((x_train * w_new + bias_new - y_train)**2)\n",
    "    \n",
    "    if new_loss <= current_loss:\n",
    "        return w_new, bias_new\n",
    "    else:\n",
    "        return w, bias\n",
    "\n",
    "fig, axes = visualize_optimization(random_search_advanced, max_iter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3.3) Gradientenabstieg\n",
    "\n",
    "Implementieren Sie nun den Gradientenabstieg. Hier der Startpunkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bdad4c60e74326a38e772f0d124a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gradient_descent(x_train, y_train, w, bias, learning_rate=0.01):\n",
    "    N = len(y_train)\n",
    "    y_pred = x_train * w + bias\n",
    "    \n",
    "    error = y_pred - y_train\n",
    "    \n",
    "    w_new = w - learning_rate * 2/N * np.sum(error * x_train)\n",
    "    bias_new = bias - learning_rate * 2/N * np.sum(error)\n",
    "    \n",
    "    return w_new, bias_new\n",
    "\n",
    "\n",
    "# die learning_rate kann von außen modifiziert werden:\n",
    "fig, axes = visualize_optimization(gradient_descent, max_iter=200, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3.4) Gradientenabstieg mit Regularisierung\n",
    "\n",
    "Implementieren Sie nun den Gradientenabstieg mit \"Weight Decay\", das heißt mit L2-Regularisierung. Hier der Startpunkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0d2700b2b145ae969dab5b9669b338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gradient_descent(x_train, y_train, w, bias, learning_rate=0.01, alpha=0.01):\n",
    "    N = len(y_train)\n",
    "    y_pred = x_train * w + bias\n",
    "    \n",
    "    error = y_pred - y_train\n",
    "    \n",
    "    w_new = w - learning_rate * 2/N * np.sum(error * x_train) - learning_rate * alpha * w\n",
    "    bias_new = bias - learning_rate * 2/N * np.sum(error)\n",
    "    \n",
    "    return w_new, bias_new\n",
    "\n",
    "\n",
    "# die learning_rate kann von außen modifiziert werden:\n",
    "fig, axes = visualize_optimization(gradient_descent, max_iter=500, learning_rate=0.01, alpha=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3.5) Der Gradientenabstieg als Machine Learning Modell\n",
    "\n",
    "Implementieren Sie ein Machine Learning Modell wie in Scikit Learn. Hier der Startpunkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self._is_fitted = False\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        \n",
    "        w = np.random.normal()\n",
    "        bias = 0.0\n",
    "        \n",
    "        # TODO: Iterative Updates von w und bias\n",
    "        \n",
    "        self.w_ = w\n",
    "        self.bias_ = bias\n",
    "        \n",
    "        N = len(y_train)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "                \n",
    "            y_pred = x_train * self.w_ + self.bias_\n",
    "\n",
    "            error = y_pred - y_train\n",
    "\n",
    "            self.w_ = self.w_ - self.learning_rate * 2/N * np.sum(error * x_train)\n",
    "            self.bias_ = self.bias_ - self.learning_rate * 2/N * np.sum(error)\n",
    "        \n",
    "        self._is_fitted = True\n",
    "        print(self.w_, self.bias_)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if not self._is_fitted:\n",
    "            raise NotFittedError\n",
    "            \n",
    "        y_pred = self.w_ * x + self.bias_\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5510388172640762 1.3518269393736975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f07f209f4d48a5bf981027e3c70218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wenn die Klasse LinearRegression richtig implementiert ist, sollte\n",
    "# der folgende Code funktionieren.\n",
    "lin_reg = LinearRegression(max_iter=1000)\n",
    "lin_reg.fit(x_train, y_train)\n",
    "\n",
    "x_vis = np.linspace(0, 10, 100)\n",
    "y_vis = lin_reg.predict(x_vis)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(x_vis, y_vis, color=\"red\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
